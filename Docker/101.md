# Docker

_stopped at 1:40_
[video](https://www.youtube.com/watch?v=fqMOX6JJhGo)




- Why use it?
Say you are using a mern stack with added stacks on that (messaging, orchestation is what they offer as example), checking the compatibilites between different libraries and operating systems gets frustating - when upgrading or swapping out a database they would have to double check all the compatibilites again - this is refered to as **the matrix from hell**. Containerizing apps can be more secure and be used to sandbox applciations so you can delete an app easier once you're not using it. 

- VMs versus Containers
	 - Containers 
	 	- Usually megabytes in size, don't take up much processing power, and quick boot times. They also have direct ways of communicating since they share the same kernel, meaning it's not fully isolated.

		How they look once built (from low level to high level):
	 	- Hardware
		- Operating System (meaning the kernal + OS)
		- Docker (or container )
	 	- The container with the "component" and it's libraries and dependencies

	- Virtual Machines 
		
		- Usually gigabytes in size, take up a lot of processing power, and slow boot times. They also have complete isolation from each other - since they use their own OS. 

		How they look once built (from low level to high level):
	 	- Hardware
		- Hypervisor (i.e. ESXi)
		- Virtual Machine which contains:
			- Operating System (meaning the kernal + OS)
			- Libraries and dependencies
			- Application

	- It's not a containers or virtual machines conversation. It's the pro and cons, and often they get combined into virtual machines running containers. With using containers, there will be less of a need of so many virtual machines, which will overall make everything a bit faster, safer, and more reliable. 


## How does Docker work?
- Docker solves hell matricies by putting each "__component__" (e.g. a node server and the mongoDB are considered different "components") in their own "__container__" which has their own libraries and dependencies for each component, siloed from the rest. 
- Each container shares the same kernel, it's what ties them together. So there are some hacky work arounds when running docker on the big three (Linux, Windows, Mac), for instance if you had a Linux container and ran it on a Windows machine, what Windows will do is take that Linux container, put it in a Linux virtual machine and then run it on the Windows kernel. This is pretty much just running VMs. 
- The purpose of Docker is to containerize your application, siloing what can be siloed,standardizing it in this process. Then allowing it to be "shipped" and installed on whatever over and over again. 

- Find an image of the machine you want to build docker on, then you can run `docker run` commands (e.g. `docker run mongodb` and `docker run nodejs`) to add as many instances of the service you need (i.e. if you for some reasons need two different nodejs install, run the command twice). If for some reason an instance fails


## Images and Docker Files

- __Note__: Every image is a based on a previous image 

-  Creating an Image (from a Dockerfile)
	- Take the steps needed to deploy your app, i.e. deploy a Python2 Flask app with a Flask MySQL database on Ubuntu Server, and then write them in a Dockerfile. The basic steps when doing this on a webserver would like this (minus, add UFW, port configs, Apache2, ect)
		- OS - Ubuntu
		- Update and upgrade via `apt`
		- Install dependencies via `apt`
		- Install other dependencies via `pip`
		- Copy the source code to /opt folder
		- Run the web server via `Flask`

	- The image only stores each layer as it goes on, this is why the containers can be so small 

	- The above steps in a Dockerfile. Notice how it is very similar to building your own server from scratch, but it does this in a more quick and automated fashion. 


- Simple Dockerfile example:
```
FROM ubuntu:latest

RUN apt-get update && apt-get -y install python

RUN pip install flask flask-mysql

COPY . /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run
```

- The Dockerfile
	- Used as a set of instructions/commands to build the image upon
	- Must be named `Dockerfile`
	- Must be a file with no extension
	- Syntax basic is INSTRUCTION/COMMMAND Argument
		- The FROM, RUN, COPY, ENTRYPOINT are all instructions for Docker to do something with the agruments passed
		- The arguments are what get run/copied/ect, this is clearly seen in `RUN apt-get install python`
	- All Dockerfiles must start with the FROM instruction, since all images are built ontop of previous images

	- Breaking down this specific Dockerfile
		- The first line is telling what the OS for the container will be built up
			- Every docker image must be based off of another image, with specified OS or a previous image
		- The RUN instructions are running CLI commands on the ubuntu image
		- The COPY instruction is copying everything in the current directory from the host to the image, with a specified location
		- The ENTRYPOINT instruction allows a command to be run when the image is run as a container

	- Once your Dockerfile is written, you can build the image from the Dockerfile
		- This will show you each of the steps and what each step is doing
		- This allows you to see where the image building fails, makes it easier to debug
		- All layers are cached by Docker, so if a step fails, all of the previous steps will be in memory - speeding up development time
	```
	$ docker build Dockerfile -t {image PATH}
	```
	- To see information on the image, i.e. how big, what the instructions are, ect, run the history on the image

	```
	$ docker history {image PATH}
	```


Docker Side Notes:
- Other container options than docker:
	- LXC (this is what docker is built on)
	- LXD
	- LXS

















































## Port Mapping
- Once a container's task is complete, the container exits. So if you want the container to persist it has to continue doing some process, e.g. running a database or hosting a server, ect

- Every Docker image is assigned an IP, but it is only accessible on the Docker host. These IP addresses are only accessible on the Docker host. If you need this to be open, you need to map the port of the container to the port of the host with the `-p` parameter, like so:

```
 $ docker run -p 80:5000 punctuationmarks/webapp
```

Cool about this is you can run different instances (read containers) of your container on different ports. So you can run a MySQL on 8306, a second MySQL on port 3306, and a Django app on 80 all on the same machine. _Note:_ You can not double up on the same port, but that shouldn't be an issue 











































## Networking

- Bridge Network 
All containers get the bridge network by default, usually in the range 172.17.0.series. Containers can access each other using this bridge IP. To access these containers to the outside world, you can map the port host (see __port mapping__)
	- Overriding the bridge network to have multiple bridges for different containers being able to communicate. Use `docker network create` to do this.  


- none Network
The container has no access to any network, meaning it's fully sandboxed

- Host Network
Uses the host's network, meaning if the port 5000 is open on the host, the container using the host network will automatically be using 5000. If using the host network, you can't run multiple containers on the same port, since each container needs its own port.  


- Containers
Find the contianer's networks and the ip, mac and gateway addresses associated with the networks under "NetwrokSettings" while inspecting the container


- Connecting containers
Connect containers with the names of the containers. The way this works is Docker has a built in DNS server that keeps track of the containers' names and their respective IP addresses. The reason is when connecting containers it is not best practice to hard code the IP addresses in the connection since if the container or server restarts then the containers might have different IP addresses.


- Built in DNS Server
Keeps track of the containers' names and IP addresses. It does this by using __network namespaces__ that creates a separate "namespace" for each container and uses __virtual ethernet__ pairs to connect the containers. By default the DNS server will be at 127.0.0.11. 







































## volume Mapping (data persistence)

- _Problem_: You have a MySQL container, you want to destroy the container but keep the database. 
- _Solution_: Use volume mapping to the container that you want the data to persist, meaning designate a folder on your host that copies all of th e data in the container, so when the container gets destroyed the data persists on the host. 

more on this [Docker docs volumn](https://docs.docker.com/storage/bind-mounts/)

```
$ docker volume create data_volume
$ docker run -v data_volume:/var/lib/mysql mysql

```






























## Interal Storage

Due to Docker building images from Dockerfiles in a __layered architecture__ every previous layer get cached, so every subsequent build that uses those already built images and doesn't need to build them again. For example, if two contianers are built from two Dockerfiles that both use ubuntu as their base Docker file, then the second container won't need to download and build the ubuntu image, it will use the previous build from the cache. 

When an image is created, it is composed of read only image layers, these are all cached. When a container is built another read/write __container layer__ is placed ontop of the read only layers, which allows for quick pivoting when the container needs updating. The container layer is destroyed once the container is destroyed, but the image layers persist until destroyed.  



File system on host:

- /var/lib/docker
	- aufs
	- image
	- volumes
		- {data_volumes}

















































## Commands and entry points in Docker
- `CMD`
Let's use an Ubuntu image as an example. When building a container, the Ubuntu container will exit upon build because its `CMD` (in the Dockerfile, meaning command) is `bash` which without a CLI will just exit. A way to keep Ubuntu running is to override the `CMD` by running a command after the image name like so. 

```
$ docker run ubuntu ls -la
```
Even in this example, once the command of listing all contents of the directory is performed, the container will exit. So how to override this? Make a custom Dockerfile that overrides the `CMD['command']` like so: _note this can be written in json or shell form_

Dockerfile with CMD
```
From ubuntu

CMD sleep 5
```

or 

```
From ubuntu

CMD["sleep"]["5" ] 
```


- `ENTRYPOINT` 
	Similar to `CMD`, but you can pass a command and then allow the parameter that is to be passed to that command to be handled by the ENTRYPOINT when the image is built into the contaier 

__Note__: If the entry point is not explicitly stated, it will default to `/bin/sh -c`. The CMD does not have a default.

Dockerfile with ENTRYPOINT instead of CMD
```
FROM ubuntu:latest

ENTRYPOINT ["sleep"]
```


```
$ sudo docker build -t testing-images/ubuntu-sleeper .
Sending build context to Docker daemon  24.06kB
Step 1/2 : FROM ubuntu:latest
 ---> 4e5021d210f6
Step 2/2 : ENTRYPOINT ["sleep"]
 ---> Running in 9a977462ae3b
Removing intermediate container 9a977462ae3b
 ---> 7d3d2e27423f
Successfully built 7d3d2e27423f
Successfully tagged testing-images/ubuntu-sleeper:latest
```

```
$ sudo docker run testing-images/ubuntu-sleeper 10
```

What if you don't pass the number of seconds? Error:
```
$ sudo docker run testing-images/ubuntu-sleeper
sleep: missing operand
Try 'sleep --help' for more information.
```
_solved with using ENTRYPOINT && CMD_

- ENTRYPOINT && CMD

Rebuild the Dockerfile with both, set the ENTRYPOINT as the command you want to run and CMD as the default value


Dockerfile:
```
FROM ubuntu:latest

ENTRYPOINT ["sleep"]

CMD ["5"]
```


Now both these work, the first has a default value of 5 seconds, the second is the overridden value of 10 seconds
```
$ sudo docker run testing-images/ubuntu-sleeper
$ sudo docker run testing-images/ubuntu-sleeper 10
```

- Another solution for the above is to have just the CMD with both command and parameter and no stated ENTRYPOINT, and when you run the container you specify the command and parameter explicity (you cannot just pass the parameter)
i.e.:















## Docker Compose
A way of writing all the commands into a .yaml file, instead of writing all of the CLi commands to build container, mounts, networks, volumns, ect. This is easier to maintain and update since it's all done in a docker-compose.yaml file, however this only works on a single docker host. So if you wanted docker-composer on multple hosts, you'd have to do that on muliple hosts

- Write the docker-compose.yaml file
- CLI command to "bring the up" the entire application stack















# "Real world" example of how to build complex contianers with Docker

- The App's general layout and stack:
	- Main app: Voting app
		- Python
	- data gets stored in an in-memory database
		- Redis
	- Data gets process by worker
		- .NET
	- Worker updates the vote and updates the persistence database
		- PostreSQL
	- The votes are displayed to end user
		- Node.js 


- Using Docker run commands 
	- Naming containers is important for ease of use and for using links
	- __Links__ are a CLI way of connecting contianers together
		- _Note:_ this is depreciated, if you have a complex app, Docker wants you to use Docker Compose


```
$ docker run -d --name=redis redis # in memory database
$ docker run -d --name=db postgres:9.4 # persistent database 
$ docker run -d --name=vote -p 5000:80 --link redis:redis voting-app # back end python app
$ docker run -d --name=result -p 5001:80 --link db:db  result-app # front end JS
$ docker run -d --name=worker --link db:db --link redis:redis  worker
```

Links in more detail

```
$ docker run -d --name={custom name} -p {host port}:{container port} --link {container name}:{host name} {image}
```

- Better to use docker-compose, allows for the step by step build of your __application stack__ with all of the containers, how they connect, their names, what ports are open, and more. Easier to use, maintain, debug, and more. All in a .yaml file that you "build up your stack" from a single CLI


For this real world example app, let's have a front end network and a back end network to keep the traffic separate


## Docker Compose Versions
	- Version 1
		- Uses the default bridge network so links are necessary

	- Version 2 and up 
		- Have more options and allow for more customized connections between containers. 
		- Main syntax differences 
			- specify which verion being used (version 1 is default)
			- put all the container builds under `services`
		- Builds a dedicated bridge network for the application and connects all the containers to that new network.
			- Allows for all containers the ability to communicate with each other with their respective names, thus eliminating links
		- Depends on feature
			- Allows for a timed procedure for how the containers are built
	- Version 3
		- Docker Swarm
		- Docker Stacks 

- Using docker-compose (version 1)
	- With all prebuilt images
```
# docker-compose.yaml 
redis: 
	image: redis
db:
	image: postgres:9.4
vote:
	image: voting-app
	ports:
		- 5000:80
	links: 
		-reddis
result:
	image: result-app
	ports:
		- 5001:80
	links:
		- db
worker:
	image: worker
	links:
		- redis
		- db  
```



- Using docker-compose (version 1)
	- With custom apps, images not built but Dockerfiles written and inside respective folders 
```
# docker-compose.yaml 
redis: 
	image: redis
db:
	image: postgres:9.4
vote:
	build: ./vote
	ports:
		- 5000:80
	links: 
		-reddis
result:
	build: ./result
	ports:
		- 5001:80
	links:
		- db
worker:
	build: ./ worker
	links:
		- redis
		- db  
```


- Using docker-compose (version 2)

```
# docker-compose.yaml 
version:2 
services:
	redis: 
		image: redis
	db:
		image: postgres:9.4
	vote:
		image: voting-app
		ports:
			- 5000:80
		depends_on:
			- redis 
	result:
		image: result-app
		ports:
			- 5001:80
	worker:
		image: worker
```

- Using docker-compose (version 3)

```
# docker-compose.yaml 
version:3
services:
	redis: 
		image: redis
	db:
		image: postgres:9.4
	vote:
		image: voting-app
		ports:
			- 5000:80
		depends_on:
			- redis 
```


- After writing the docker-compose.yaml, "bring up the stack" with 
```
$ docker compose up
```



















# Vocab:

__Containers__: Completely siloed environments, they can have their own processes or services, network interfaces and mounts - just like virtual machines, minus all the containers share the same operating system kernel (or "underlining kernel"). Containers run instances of images. 

___OS Kernel__: Responsible for interacting with the underlining hardware, the kernal can be the same and have different "operating systems" based on the software used to make the GUI or CLI (i.e. think Ubuntu vs Mint, they both are built on the same linux kernel, but are drastically different)

 
__Image__: A template for creating containers. It can be thought of as the instructions for how to set up the environment (the container) for a specific app. 

___Dockerfile__: Set of instructions on how to create the image, for that app on that OS

__Instance__: The service you install with docker, e.g. running `docker run nodejs` installs the "instance" of nodejs into that container


#### Docker specific vocab


- In general:
__NAME__: a randomly generated name for a container, in english
__CONTAINER ID__:  a randomly generated id for a container, looks like some hex code
__STATUS__: _up_ or _exited_ meaning the container is running or not
__TAG__ : Any extra info given to an image when building a container like adding a specific version, so it's not the latest version. 
__Host__: The computer that is running Docker
__Docker Host/Docker Engine__: The underlining host where Docker is installed

__Layered architecture__: In Docker this means that each __image layer__ is built independently and cached, so if there is a failure in building a container, it will only continue building from where the step failed. The image layers are then written into a read only file

__Copy-on-Write mechanism__: When an image with an app is created and you want to update the app sometime in the future, Docker copies the app files and allows read/write abilities on the contianer layer. Hence it "copies" files on "writing" the container. 

__Volume mounting__: Persisting data from a container onto the host via making volumes
__Bind mounting__: Same as volume mounting, but the path is specified and not mounted under the volumes/ folder
__docker swarm__: Having multiple "replicas" or copies of the same container from the same image on a host

- In the Dockerfile:
__FROM__: Where the image is pulling its _base_ image from 
___ENTRYPOINT__: A command that can be run once the image is run as a container
___RUN__: Runs the command on the image when building a container
__COPY__: Allows copying from the host to the image





















































# Tips:

- Best practices
	- Docker containers are not little virtual machines
		- Don't abuse them
	- Containers are meant to isolate processes, not services
	- Container orchestration systems expect containers to ephemeral, meaning if they fail, and they're expected to fail, they fail fast



- Trouble Shooting and Debugging:

	- If you run into issues downloading an image, it might be your connectivity to their network. Some reason it doesn't like when you use a VPN (which makes me not like it, but maybe it's not a scary thing and just poor connectivity or oversight). This might not happen with all VPNs or VPN connections


- Finding images
[Docker.com's most popular images](http://hub.docker.com/)

- Each image on hub.docker will have 

- Installing Docker

	[Install docker community edition based on your OS ](https://docs.docker.com/)

	- Uninstall an older version if one exists
	- You can automate the installation of Docker using their _convient script_
		- The first command downloads the script
		- The second installs all the commands on the script. 
	```
	$ curl -fsSL https://get.docker.com -o get-docker.sh
	$ sudo sh get-docker.sh
	# Executing docker install script, commit: 442e66405c304fa92af8aadaa1d9b31bf4b0ad94
	+ sh -c apt-get update -qq >/dev/null
	...

	```


	- You can also install it the old fashioned way as well by adding the respository, ect. Check their docs. 

- Specifying the container ID can be done with just the first few letters since the hashes are unique (maybe to be safe use five, but if every hash starts with a different character you can literally run them off one character)

- Current tags and their linked docker files are dispalyed on the image's page on hub.docker 












































# Docker Syntax Cheatsheet

- Version _running it without sudo will give less information_

```
$ sudo docker version
```

- A silly way of testing install was alright 
	- This is image is from hub.docker.com 
	- It'll automatically pull the image and run it in one command

```
$ sudo docker run docker/whalesay cowsay yoyoyo
Unable to find image 'docker/whalesay:latest' locally
latest: Pulling from docker/whalesay
Image docker.io/docker/whalesay:latest uses outdated schema1 manifest format. Please upgrade to a schema2 image for better future compatibility. More information at https://docs.docker.com/registry/spec/deprecated-schema-v1/
e190868d63f8: Pull complete 
909cd34c6fd7: Pull complete 
0b9bfabab7c1: Pull complete 
a3ed95caeb02: Pull complete 
00bf65475aba: Pull complete 
c57b6bcc83e3: Pull complete 
8978f6879e2f: Pull complete 
8eed3712d2cf: Pull complete 
Digest: sha256:178598e51a26abbc958b8a2e48825c90bc22e641de3d31e18aaf55f3258ba93b
Status: Downloaded newer image for docker/whalesay:latest
 ________ 
< yoyoyo >
 -------- 
    \
     \
      \     
                    ##        .            
              ## ## ##       ==            
           ## ## ## ##      ===            
       /""""""""""""""""___/ ===        
  ~~~ {~~ ~~~~ ~~~ ~~~~ ~~ ~ /  ===- ~~~   
       \______ o          __/            
        \    \        __/             
          \____\______/  
```


- "Pulling" images from hub.docker (you can think of this like using a package manager)

```
$ docker pull docker/whalesay

```
- Running an image. If the image is not on the host (meaning it hasn't be built there or has not been pulled), it'll go out and pull the image from hub.docker.com
```
$  docker run nginx
```

- Running an image with a tag specifying the version (_note the default tag is "latest"_ if not specified) 
```
$ docker run nodejs:10.20.0-jessie
```

- Running an image with specified application function and input into that function
```
$ docker run docker/whalesay cowsay yoyoyo
```

- Listing all of the running containers and their info

```
$ docker ps

```
- Listing all of the containers, running or not

```
$ docker ps -a
```

- To stop a container
```
$ docker stop {container}
```

- Listing all of the images on the host
```
$ dockers images
```


- Removing a specific container (has to be stopped)
```
$ docker rm {container}
```

- Removing an image 
	- **Note:** very important that no containers are running off of that image!
	- So make sure all containers are stopped and deleted before deleting an image 
```
$ docker rmi {image}
```

- Deleting any images, containers, volumes and networks that are not associated directly with container (docker calls these **dangling**)
```
$ docker system prune
```

- Deleting any stopped containers, unused images, and anything **dangling**
```
$ docker system prune -a
```

- Executing a command when you run the container
```
$ docker run {image} {command}
```

- Executing commannds on running containers
```
$ docker exec {container} {command}
```

- Run with detach (aka running the container in the background)
	- This is so you can keep using the CLI when you created the container

```
$ docker run -d {image}
```

- Attaching a detached container
```
$ docker attach {container}
```

- Naming a container on run
```
$ docker --name {custom container name} {image}
```

- Mapping ports to allow app access outside

```
$ docker run -p 80:5000 punctuationmarks/webapp
```

- volume mapping from starting the container
```
$ docker run -v {path on host}:{path on container} {image}
```

- Set environmental variables at the same time as running a container
```
$ docker run -e ENV_VAR=var {image}
```


- Inspect a specific container which returns json with a ton more info than `ps`
	- You can see what ports are being acccessed as well as what evironment variables are set here too. Lots of info
```
$ docker inspect {container}
```

- View the logs of a container
```
$ docker logs {container}
```

- Renaming a container after it is already built
```
$ docker rename {original name} {new name}
```


- Building own image from custom Dockerfile, this make the image available locally. The docker file can be named whatever, but by convention the file is named `Dockerfile`
	- This will print each of the steps and what each step is doing 
```
$ docker build {Dockerfile} -t {image tag}
```

- To see information on the image, i.e. how big, what the instructions are, ect

```
$ docker history {image PATH}
```

- Pushing an image from your local machine to hub.docker
```
$ docker push {PATH to image}
```

- Standard input mode for the docker container (note, this will allow access to input but not show any prompt)
```
$ docker run -i {image}
```

- Standard input mode with terminal prompt
```
$ docker run -it {image}
```

-  Run Bridge (default) network
```

$ docker run {image}
```

- Null Network
```

$ docker run {image} --network=none
```


- Run Host network
```

$ docker run {image} --network=host
```

- Listing all networks
```
$ docker network ls
```

- Attaching a container to a specific network
```
$ docker run -d --network {network name} {image}
```

- Creating another bridge network with 182.18.0. 1 to 16 series as an example 
```
docker network create --driver bridge --subnet 182.18.0.1/16 {new bridge name}

```

- Creating another bridge network with 182.18.0. 1 to 16 series with a specified gateway of 182.18.0.1 as an example 
```
docker network create --driver bridge --subnet 182.18.0.1/16 --gateway 182.18.0.1 {new bridge name}

```


- Creating volume on host
	- Will be stored under volumes/ by default, but this can be overwritten to have the path specified if so desired
```
$ docker volume create {PATH to folder name}

```


- Using the created volume to persist data from a container AKA mounting
	- _Note_: Docker will create the volume folder on host under volume/ if it was not created earilier with the `volume create` command, the path can also be overridden (when it's overriden it's called bind mounting instead of volume mounting if overriden)
```
$ docker run -v {volume folder on host}:{container's data you want to copy} {image}
```

- More verbose way of mounting, with explicitly stating the type, source, and target
```
$ docker run --mount type=bind,source={volume folder on host},target={container's data you want to copy} {image}
```

- Limit the percentage of cpu use on a host by a container (50% in this example)
```
$ docker run --cpus=.5 {image}
```


- Limit the amount of memory use on a host by a container (100 megabytes in this example)
```
$ docker run --memory=100m {image}
```


- Using container orchestration to create muliple copies of a container
	- Called __docker swarm__
```
$ docker service create --replicas={integer} {image} 
```


- Initializing docker swarm manager
	- Has to be run on docker's manager/master host
	- This will print a token to be used for the workers/slaves
```
$ docker swarm init
```

- Joining the workers/slaves to the manager/master
	 - Do this on all worker/slave hosts
```
$ docker swarm join --token {token}
```
















__Compound command examples__

- Example of renaming a container, detaching the container and running a command when the container is created.  

- Then running a command on that contianer
```
$ sudo docker run -d --name something_memorable ubuntu sleep 100
[sudo] password for punctuationmarks: 
2155885f58ff4917b3ad61601345933fad6cd6f722c8a6829009a34f05a9733d

$ sudo docker ps
CONTAINER ID        image               COMMAND             CREATED             STATUS              PORTS               NAMES
2155885f58ff        ubuntu              "sleep 100"         7 seconds ago       Up 4 seconds                            something_memorable
```

- Then running a command on that contianer

```
$ sudo docker exec something_memorable ls -la
total 72
drwxr-xr-x   1 root root 4096 Apr  9 16:35 .
drwxr-xr-x   1 root root 4096 Apr  9 16:35 ..
-rwxr-xr-x   1 root root    0 Apr  9 16:35 .dockerenv
drwxr-xr-x   2 root root 4096 Mar 11 21:05 bin
drwxr-xr-x   2 root root 4096 Apr 24  2018 boot
drwxr-xr-x   5 root root  340 Apr  9 16:35 dev
drwxr-xr-x   1 root root 4096 Apr  9 16:35 etc
drwxr-xr-x   2 root root 4096 Apr 24  2018 home
drwxr-xr-x   8 root root 4096 May 23  2017 lib
drwxr-xr-x   2 root root 4096 Mar 11 21:03 lib64
drwxr-xr-x   2 root root 4096 Mar 11 21:03 media
drwxr-xr-x   2 root root 4096 Mar 11 21:03 mnt
drwxr-xr-x   2 root root 4096 Mar 11 21:03 opt
dr-xr-xr-x 416 root root    0 Apr  9 16:35 proc
drwx------   2 root root 4096 Mar 11 21:05 root
drwxr-xr-x   1 root root 4096 Mar 20 19:20 run
drwxr-xr-x   1 root root 4096 Mar 20 19:20 sbin
drwxr-xr-x   2 root root 4096 Mar 11 21:03 srv
dr-xr-xr-x  13 root root    0 Apr  9 16:35 sys
drwxrwxrwt   2 root root 4096 Mar 11 21:05 tmp
drwxr-xr-x   1 root root 4096 Mar 11 21:03 usr
drwxr-xr-x   1 root root 4096 Mar 11 21:05 var


```


- Creating a container from an image of punctuationmarks/webapp, with a tag of 1.01, running it on the host's port of 38282 from the container's port of 8080

```
$ sudo docker run -p 38282:8080 punctuationmarks/webapp:1.01
```


- Seeing the image's history on how it was built and more info:
```
$ sudo docker history testing-images/flask-image

image               CREATED             CREATED BY                                      SIZE                COMMENT
2e09ef39a8a6        2 minutes ago       /bin/sh -c #(nop)  ENTRYPOINT ["/bin/sh" "-c…   0B                  
228af7f3d262        2 minutes ago       /bin/sh -c #(nop) COPY dir:127b3e3089b1d0fe3…   17.5kB              
488a17739480        2 minutes ago       /bin/sh -c pip3 install flask flask-mysql       4.71MB              
111d3fa423ff        2 minutes ago       /bin/sh -c apt-get update && apt-get -y inst…   412MB               
4e5021d210f6        2 weeks ago         /bin/sh -c #(nop)  CMD ["/bin/bash"]            0B                  
<missing>           2 weeks ago         /bin/sh -c mkdir -p /run/systemd && echo 'do…   7B                  
<missing>           2 weeks ago         /bin/sh -c set -xe   && echo '#!/bin/sh' > /…   745B                
<missing>           2 weeks ago         /bin/sh -c [ -z "$(apt-get indextargets)" ]     987kB               
<missing>           2 weeks ago         /bin/sh -c #(nop) ADD file:594fa35cf803361e6…   63.2MB              
``` 


- Running a mysql container with a tag of 5.6, naming that container "mysql-db", running the container detatched to then set it on a network called "wp-mysql-network", using the environment variable to set the MySQL's root password to "db_pass123". 
_Note how the syntax is, first we're running a detatched something, then setting its environment varable, then the container's name, then specifying where to attach the container, then specifying which image and associated tags to build the container_
```
$ docker run -d -e MYSQL_ROOT_PASSWORD=db_pass123 --name mysql-db --network wp-mysql-network mysql:5.6
```


- Deploy a web application named webapp, using image kodekloud/simple-webapp-mysql. Expose port to 38080 on the host. The application takes an environment variable DB_Host that has the hostname of the mysql database. Make sure to attach it to the newly created network wp-mysql-network

```
docker run --network=wp-mysql-network -e DB_Host=mysql-db -e DB_Password=db_pass123 -p 38080:8080 --name webapp --link mysql-db:mysql-db -d kodekloud/simple-webapp-mysql
```




- Yaml file with front and backend networks with complex build
	- Having the back end as remote as possible, with the voting app and the results app be in both the frontend and the backend network, but the databases are just in the backend network

```
# docker-compose.yaml 
version:2 
services:
	redis: 
		image: redis
		networks:
			- back-end		
	db:
		image: postgres:9.4
		networks:
			- back-end		
	vote:
		image: voting-app
		ports:
			- 5000:80
		depends_on:
			- redis 
		networks:
			- front-end
			- back-end			
	result:
		image: result-app
		ports:
			- 5001:80
		networks:
			- front-end
			- back-end
	worker:
		image: worker

networks:
	front-end:
	back-end: 
```



- Building a Docker container for Django [from tutorial](https://www.codingforentrepreneurs.com/blog/django-on-docker-a-simple-introduction)



- Example Dockerfile for simple django app with Gunicorn
```

  GNU nano 2.9.3                                       Dockerfile                                                  

# Base Image
FROM python:3.6

# create and set working directory
RUN mkdir /app
# explicitly stating this is the working directory
WORKDIR /app

# Add current directory code to working directory
# /app is arbitrary, can be named whatever
# COPY doesn't work as well as ADD  currently
ADD . /app/

# set default environment variables
ENV PYTHONUNBUFFERED 1
ENV LANG C.UTF-8
ENV DEBIAN_FRONTEND=noninteractive

# set project environment variables
# grab these via Python's os.environ
# these are 100% optional here
ENV PORT=8888
# setting environment variables, can set the debug to false here like so (for production)
# ENV DEBUG=0

# Install system dependencies
RUN apt-get update && apt-get install -y --no-install-recommends \
        tzdata \
        python3-setuptools \
        python3-pip \
        python3-dev \
        python3-venv \
        git \
        && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/*


# install environment dependencies
RUN pip3 install --upgrade pip
# RUN pip3 install pipenv

# Install project dependencies
# don't need a virtual environment since it's its own container, but 
# you could create, activate and install dependencies inside of a virtual environment
# optionally, with pip3 freeze
# RUN pip3 install -r requirements.txt
RUN pip3 install django gunicorn

# expose whatever port is declared in the environment variable
EXPOSE 8888
# the command here is grabbing the $PORT that is the variable above that is ENV PORT=....
CMD gunicorn django_docker.wsgi:application --bind 0.0.0.0:$PORT


```

- Creating the docker image from the Dockerfile
	- dash t for the tag given to the image
	- dash f for explicitly stating which file to build the image from 
	- period is the path of cwd
```
$ sudo docker build -t docker-django-vanilla -f Dockerfile .
```


- Creating the container from the image
	- Dash it stands for iteractive terminal
	- Port at 80 is a standard, it is the hosts port, so this is being displayed on the 0.0.0.0 or localhost website (port 80 can be declared, but it is implied)
	- Port 8888 is the port declared to run the Django app
```
$ sudo docker run -it -p 80:8888 docker-django-vanilla
[2020-04-12 16:48:34 +0000] [6] [INFO] Starting gunicorn 20.0.4
[2020-04-12 16:48:34 +0000] [6] [INFO] Listening at: http://0.0.0.0:8888 (6)
[2020-04-12 16:48:34 +0000] [6] [INFO] Using worker: sync
[2020-04-12 16:48:34 +0000] [9] [INFO] Booting worker with pid: 9
Invalid HTTP_HOST header: '0.0.0.0'. You may need to add '0.0.0.0' to ALLOWED_HOSTS.
Bad Request: /
Invalid HTTP_HOST header: '0.0.0.0'. You may need to add '0.0.0.0' to ALLOWED_HOSTS.
Bad Request: /favicon.ico
Not Found: /static/admin/css/fonts.css
Not Found: /favicon.ico
Not Found: /static/admin/css/fonts.css
^C[2020-04-12 16:58:17 +0000] [6] [INFO] Handling signal: int
[2020-04-12 16:58:17 +0000] [9] [INFO] Worker exiting (pid: 9)
[2020-04-12 16:58:17 +0000] [6] [INFO] Shutting down: Master


```



- Running it in detached mode so it just stays us
	- Dash d runs the contianer in the background
	- Placement is important of "flags"
	- Note, this will continually run, so make sure to stop the container
```
$ sudo docker run -it -d -p 80:8888 docker-django-vanilla

```



















## Deeper dives

- Docker Registry
	- Docker.io is assumed, and probably largest
	- Google has kubernetes testing images
	- Some cloud providers (AWS, GCP) give a private registry
	- Can built your own private registry (there's an image for that)


- Docker Engine
	- Docker Engine is simply a host with docker installe don it. 
	- When installing Docker on a host, what is actually being install is the Docker CLI, Docker REST API, and Docker Deamon. 
	
	- Structure
		- Docker CLI (_Note_ this doesn't _have_ to be on the same host, ie using a remote docker engine) 
		- REST API (used to interact with the deamon)
		- Docker Deamon



 - Containerization
	- Namespaces are used for everything, (mount, unix timesharing, process ID, network, InterProcess). This is used for containers to be in isolation. Example, if you have Docker on a Linux host, and you run a container, the Linux host will have process IDs starting at 1 and incrementing on every subsequent process, the container needs to think it's starting at process ID 1 as well, since it's a container sharing the same OS kernel, namespaces help solve this problem since process IDs cannot have the same number. So the contianer gets the next available process IDs in the Linux line, and it looks to the container that it's starting at 1, which solves both problems, with namespaces. 

- Cgroups and resource sharing
	- The containers share the same resources of the host, and by default there is no max of how much the containers can use. 
	- Docker uses cgroups (control groups) to limit the access of hardware the containers use. This can be set in percentages or set amounts, depending on the need and what is being limited.

- Orchestrate
	- A set of tools and scripts that can help keep your applications running on multiple docker hosts containing multple docker containers
	- Allows for automating scaling up and scaling down the number of users as needed, aka __load balancing__
	- Can even allow the addition of the number of hosts, can even get more advanced and help with networking between the hosts and containers 
	- Networking and security
	- Some of the main solutions to these problems:
		- Docker Swarm (easy to get started, not super flexible)
		- Google's kubernetes (hard to get started, very flexible, most popular)
		- Apache's MESOS (hard to get started, very flexible)



- Docker Swarm
	- Have multiple hosts (machines with Docker installed)
	- Designate one host to be the "Swarm Manager" (aka master)
	- All other hosts are "workers" (aka slaves)
	
	- Setting up an application across the cluster of hosts
		- On the master host, create a service, then state how many "replicas" you want, which in this context mean how many slave hosts there are. You can pass environment variables, ports, set up networks, pretty much everything under `docker service create` with a master and slaves hosts


- Kubernetes
	- Allows a ridiculous number of replicas of the same image on bunch of nodes. Meaning, one master, lots of slaves, and multiple instances on each slave. This can be done to scale up and down based on user load. Can even do upgrades on each of the nodes, in a rolling fashion. Allows for rollbacks to previous builds as well. Allows optional upgrading only a percentage with AB testing methods. Kubernetes is open sourced, and has pretty much every plug in available. Kubernetes uses any container, just Docker is common. A node is a host with containers. A cluster is a group of nodes.  

	- What gets installed with kubernetes 
		- API server
		- etcd
		- kubelet
		- container runtime
		- controller
		- scheduler
		
	
	- Things to research to learn more:
		- kubectl 
		- POD AutoScalers
		- Cluster AutoScalers
		- rolling-update
		- --rollback
